# -*- coding: utf-8 -*-
"""MohammadAminKiani_4043644008.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dkvfKbAr4euGjmB3rlizF9k388vSOho6

# Mohammad Amin Kiani 4043644008

## pc eng - AI 404

### NN-ex1 . ui.ac.ir

## 0- Needs
"""

!pip install -q datasets torchvision tensorboard

import os
import random
import pickle
from copy import deepcopy

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

from torchvision import transforms
import matplotlib.pyplot as plt

from datasets import load_dataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# numb normalize ImageNet
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

NUM_CLASSES = 3  # healthy, angular_leaf_spot, rust
INPUT_SIZE  = (3, 224, 224)
FLATTEN_DIM = 3 * 224 * 224
print("Flatten dim =", FLATTEN_DIM)

from google.colab import drive
drive.mount('/content/drive')

CHECKPOINT_DIR = "/content/drive/MyDrive/checkpoints"  # مسیر ذخیره چک‌پوینت‌ها
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

"""## 1- loading"""

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

#  train / validation / test
hf_ds = load_dataset("AI-Lab-Makerere/beans")
print(hf_ds)
print(hf_ds["train"][0])

class AddGaussianNoise(nn.Module):
    def __init__(self, mean=0.0, std=1.0):
        super().__init__()
        self.mean = mean
        self.std = std

    def forward(self, tensor):
        if self.std == 0:
            return tensor
        noise = torch.randn_like(tensor) * self.std + self.mean
        return tensor + noise


def get_transforms(use_normalization: bool = True,
                   add_gaussian_noise_to_test: bool = False,
                   noise_std: float = 1.0):
    """
    برای train/val/test سه ترنسفورم برمی‌گرداند.
    """
    base = [transforms.Resize((224, 224)), transforms.ToTensor()]

    # train/val
    train_tf = base.copy()
    val_tf   = base.copy()
    test_tf  = base.copy()

    if use_normalization:
        norm = transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)
        train_tf.append(norm)
        val_tf.append(norm)
        test_tf.append(norm)

    # فقط آزمایش ۵: افزودن نویز گاوسی به تصاویر test
    if add_gaussian_noise_to_test:
        # نویز بعد از ToTensor (روی بازه [0,1]) قبل از نرمال‌سازی بودن/نبودن مهم نیست،

        test_tf.append(AddGaussianNoise(mean=0.0, std=noise_std))

    return (
        transforms.Compose(train_tf),
        transforms.Compose(val_tf),
        transforms.Compose(test_tf),
    )

class BeansTorchDataset(Dataset):
    def __init__(self, hf_split, transform=None, override_labels=None):
        """
        hf_split: یکی از hf_ds["train"], hf_ds["validation"], hf_ds["test"]
        override_labels: اگر لیستی از لیبل‌ها بدهیم، از آن استفاده می‌شود
                         (برای آزمایش ۴).
        """
        self.hf_split = hf_split
        self.transform = transform
        self.override_labels = override_labels

    def __len__(self):
        return len(self.hf_split)

    def __getitem__(self, idx):
        example = self.hf_split[idx]
        img = example["image"]   # PIL
        label = example["labels"]

        if self.override_labels is not None:
            label = int(self.override_labels[idx])

        if self.transform is not None:
            img = self.transform(img)
        return img, label

from collections import Counter

label_counts = Counter()

for split_name in ["train", "validation", "test"]:
    split = hf_ds[split_name]
    labels = [ex["labels"] for ex in split]
    label_counts.update(labels)


# نگاشت عدد لیبل‌ها HF:
# 0 -> لکه‌ی زاویه‌ای برگ (angular_leaf_spot)
# 1 -> قارچ زنگار (bean_rust)
# 2 -> برگ سالم (healthy)


# 0: angular_leaf_spot, 1: bean_rust, 2: healthy
id2label = {0: "angular_leaf_spot", 1: "bean_rust", 2: "healthy"}
for i in range(NUM_CLASSES):
    print(f"class {i} ({id2label[i]}): {label_counts[i]} samples (تمام splits)")

# ترنسفورم با نرمال‌سازی برای نمایش "بعد"
_, _, test_tf = get_transforms(use_normalization=True)

# برای نمایش، یک تابع برای Unnormalize می‌سازیم
inv_norm = transforms.Normalize(
    mean=[-m / s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD)],
    std=[1/s for s in IMAGENET_STD]
)

def show_tensor_image(tensor, title=None, unnormalize=False):
    if unnormalize:
        tensor = inv_norm(tensor)
    img = tensor.detach().cpu().clamp(0, 1)
    img = np.transpose(img.numpy(), (1, 2, 0))
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis("off")

# برای هر کلاس، یک نمونه تصادفی از کل دیتاست (train/val/test) پیدا می‌کنیم
def find_random_sample_for_class(label_id):
    indices = []
    for split_name in ["train", "validation", "test"]:
        split = hf_ds[split_name]
        for i in range(len(split)):
            if split[i]["labels"] == label_id:
                indices.append((split_name, i))
    split_name, idx = random.choice(indices)
    return split_name, idx

plt.figure(figsize=(9, 6))
plot_idx = 1

for c in range(NUM_CLASSES):
    split_name, idx = find_random_sample_for_class(c)
    ex = hf_ds[split_name][idx]
    pil_img = ex["image"]

    # قبل از نرمال‌سازی
    plt.subplot(2, NUM_CLASSES, plot_idx)
    plt.imshow(pil_img)
    plt.title(f"class {c} ({id2label[c]}) - before")
    plt.axis("off")

    # بعد از نرمال‌سازی (برای نمایش، Unnormalize می‌کنیم که رنگ‌ها قابل دیدن باشد)
    tensor_img = test_tf(pil_img)
    plt.subplot(2, NUM_CLASSES, plot_idx + NUM_CLASSES)
    show_tensor_image(tensor_img, title=f"class {c} ({id2label[c]}) - after", unnormalize=True)
    plot_idx += 1

plt.tight_layout()
plt.show()

class BeanMLP(nn.Module):
    def __init__(self, input_dim=FLATTEN_DIM,
                 hidden1=512, hidden2=256,
                 num_classes=NUM_CLASSES,
                 p1=0.3, p2=0.3):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(input_dim, hidden1)   # W1: 512 × 150528
        self.fc2 = nn.Linear(hidden1, hidden2)     # W2: 256 × 512
        self.fc3 = nn.Linear(hidden2, num_classes) # W3: 3 × 256
        self.dropout1 = nn.Dropout(p=p1)
        self.dropout2 = nn.Dropout(p=p2)

    def forward(self, x):
        # x: (B, 3, 224, 224)
        x = self.flatten(x)
        o1 = F.relu(self.fc1(x))
        o1 = self.dropout1(o1)
        o2 = F.relu(self.fc2(o1))
        o2 = self.dropout2(o2)
        logits = self.fc3(o2)  # softmax را به CrossEntropyLoss
        return logits

"""## 2- train - test - val"""

def make_dataloaders(use_normalization=True,
                     batch_size=32,
                     shuffle_train=True,
                     random_labels_for_train_val=False,
                     add_gaussian_noise_to_test=False,
                     noise_std=1.0):

    train_tf, val_tf, test_tf = get_transforms(
        use_normalization=use_normalization,
        add_gaussian_noise_to_test=add_gaussian_noise_to_test,
        noise_std=noise_std
    )

    # برچسب تصادفی فقط برای آزمایش ۴
    train_override_labels = None
    val_override_labels = None

    if random_labels_for_train_val:
        train_len = len(hf_ds["train"])
        val_len   = len(hf_ds["validation"])
        train_override_labels = np.random.randint(0, NUM_CLASSES, size=train_len)
        val_override_labels   = np.random.randint(0, NUM_CLASSES, size=val_len)

    train_dataset = BeansTorchDataset(
        hf_split=hf_ds["train"],
        transform=train_tf,
        override_labels=train_override_labels
    )
    val_dataset = BeansTorchDataset(
        hf_split=hf_ds["validation"],
        transform=val_tf,
        override_labels=val_override_labels
    )
    test_dataset = BeansTorchDataset(
        hf_split=hf_ds["test"],
        transform=test_tf,
        override_labels=None  # never do
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train, num_workers=2, pin_memory=True)
    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

    return train_loader, val_loader, test_loader

def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        logits = model(images)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        preds = logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    epoch_loss = running_loss / total
    epoch_acc  = correct / total
    return epoch_loss, epoch_acc


@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in loader:
        images = images.to(device)
        labels = labels.to(device)

        logits = model(images)
        loss = criterion(logits, labels)

        running_loss += loss.item() * images.size(0)
        preds = logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    epoch_loss = running_loss / total
    epoch_acc  = correct / total
    return epoch_loss, epoch_acc

@torch.no_grad()
def evaluate_with_tta(model, loader, criterion, tta_transforms, n_augmentations=None):
    """
    ارزیابی مدل با Test-Time Augmentation
    tta_transforms: لیستی از ترنسفورم‌ها که روی تانسور (C,H,W) کار می‌کنند.
    n_augmentations: اگر None باشد از len(tta_transforms) استفاده می‌شود.
    """
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    if n_augmentations is None:
        n_augmentations = len(tta_transforms)
    # فقط به تعداد موردنظر از ترنسفورم‌ها استفاده می‌کنیم
    tta_transforms = tta_transforms[:n_augmentations]

    for images, labels in loader:
        batch_size = images.size(0)
        labels = labels.to(device)

        # (n_aug, B, C, H, W) می‌سازیم
        aug_list = []
        for t in tta_transforms:
            # t روی هر تصویر جداگانه اعمال می‌شود
            aug = torch.stack([t(img) for img in images])  # شکل: (B, C, H, W)
            aug_list.append(aug)

        images_tta = torch.stack(aug_list, dim=0)  # شکل: (n_aug, B, C, H, W)
        n_aug, B, C, H, W = images_tta.shape

        # انتقال به device
        images_tta = images_tta.to(device)

        # فوروارد: (n_aug * B, C, H, W) → (n_aug, B, num_classes)
        images_flat = images_tta.view(n_aug * B, C, H, W)
        logits = model(images_flat)
        logits = logits.view(n_aug, B, -1)

        # میانگین‌گیری روی augmentations
        logits_mean = logits.mean(dim=0)  # (B, num_classes)

        loss = criterion(logits_mean, labels)
        running_loss += loss.item() * B
        preds = logits_mean.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += B

    epoch_loss = running_loss / total
    epoch_acc  = correct / total
    return epoch_loss, epoch_acc

def save_checkpoint(model, optimizer, epoch, loss, acc, filename):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'accuracy': acc,
    }
    torch.save(checkpoint, filename)
    print(f"چک‌پوینت در {filename} ذخیره شد.")


def load_checkpoint(model, optimizer, filename):
    checkpoint = torch.load(filename)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    accuracy = checkpoint['accuracy']
    print(f"چک‌پوینت بارگذاری شد: {filename} - Epoch {epoch} - Loss {loss} - Accuracy {accuracy}")
    return model, optimizer, epoch, loss, accuracy

def run_single_training(config, exp_name, run_idx):
    """
    یک ران (اجرا) کامل برای یک آزمایش.
    """
    set_seed(config["seed_base"] + run_idx)

    train_loader, val_loader, test_loader = make_dataloaders(
        use_normalization=config["use_normalization"],
        batch_size=config["batch_size"],
        shuffle_train=True,
        random_labels_for_train_val=config["random_labels_for_train_val"],
        add_gaussian_noise_to_test=config["add_gaussian_noise_to_test"],
        noise_std=config["noise_std"],
    )

    model = BeanMLP(
        input_dim=FLATTEN_DIM,
        hidden1=config["hidden1"],
        hidden2=config["hidden2"],
        num_classes=NUM_CLASSES,
        p1=config["dropout_p1"],
        p2=config["dropout_p2"],
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=config["lr"])
    criterion = nn.CrossEntropyLoss()

    log_dir = os.path.join("runs", f"{exp_name}_run{run_idx}")
    writer = SummaryWriter(log_dir=log_dir)

    best_val_loss = float("inf")
    best_state_dict = None
    best_epoch = -1
    best_metrics_at_best = {}

    history = {
        "train_loss": [],
        "train_acc": [],
        "val_loss": [],
        "val_acc": [],
        "test_loss": [],
        "test_acc": [],
    }

    # checkpoint_filename = "/content/drive/MyDrive/checkpoints/checkpoint_epoch_X.pth"
    # model, optimizer, start_epoch, start_loss, start_acc = load_checkpoint(model, optimizer, checkpoint_filename)

    for epoch in range(config["num_epochs"]):
        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)
        val_loss, val_acc     = evaluate(model, val_loader, criterion)
        test_loss, test_acc   = evaluate(model, test_loader, criterion)

        # checkpoint_filename = os.path.join(CHECKPOINT_DIR, f"checkpoint_epoch_{epoch+1}.pth")
        # save_checkpoint(model, optimizer, epoch, val_loss, val_acc, checkpoint_filename)

        history["train_loss"].append(train_loss)
        history["train_acc"].append(train_acc)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)
        history["test_loss"].append(test_loss)
        history["test_acc"].append(test_acc)

        print(f"[{exp_name} | run {run_idx} | epoch {epoch+1}/{config['num_epochs']}] "
              f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, "
              f"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}, test_acc={test_acc:.3f}")

        # لاگ برای tensorboard
        global_step = epoch
        writer.add_scalars("loss", {
            "train": train_loss,
            "val": val_loss,
            "test": test_loss,
        }, global_step)
        writer.add_scalars("accuracy", {
            "train": train_acc,
            "val": val_acc,
            "test": test_acc,
        }, global_step)

        # بهترین مدل بر اساس val_loss
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            best_state_dict = deepcopy(model.state_dict())
            best_metrics_at_best = {
                "train_loss": train_loss,
                "train_acc": train_acc,
                "val_loss": val_loss,
                "val_acc": val_acc,
                "test_loss": test_loss,
                "test_acc": test_acc,
            }

    writer.close()

    # بازگرداندن خلاصه ران
    run_summary = {
        "best_val_loss": best_val_loss,
        "best_epoch": best_epoch,
        "best_state_dict": best_state_dict,
        "best_metrics": best_metrics_at_best,
        "history": history,
    }
    return run_summary

def run_experiment(exp_name, base_config, num_runs=20, save_pickle=True):
    """
    یک آزمایش را num_runs بار اجرا می‌کند و:
     - میانگین خطا و دقت train/val/test روی بهترین مدل هر ران را حساب می‌کند
     - بهترین مدل در تمام ران‌ها را در یک pickle ذخیره می‌کند
    """
    all_train_loss = []
    all_train_acc  = []
    all_val_loss   = []
    all_val_acc    = []
    all_test_loss  = []
    all_test_acc   = []

    best_overall = None
    best_overall_val_loss = float("inf")
    best_overall_run_idx = -1

    for run_idx in range(num_runs):
        print("=" * 80)
        print(f"شروع آزمایش {exp_name} - ران {run_idx}")
        config = deepcopy(base_config)
        config["seed_base"] = base_config.get("seed_base", 42)

        run_summary = run_single_training(config, exp_name, run_idx)

        m = run_summary["best_metrics"]
        all_train_loss.append(m["train_loss"])
        all_train_acc.append(m["train_acc"])
        all_val_loss.append(m["val_loss"])
        all_val_acc.append(m["val_acc"])
        all_test_loss.append(m["test_loss"])
        all_test_acc.append(m["test_acc"])

        if run_summary["best_val_loss"] < best_overall_val_loss:
            best_overall_val_loss = run_summary["best_val_loss"]
            best_overall_run_idx = run_idx
            best_overall = {
                "exp_name": exp_name,
                "run_idx": run_idx,
                "config": config,
                "model_state_dict": run_summary["best_state_dict"],
                "best_val_loss": run_summary["best_val_loss"],
                "best_epoch": run_summary["best_epoch"],
                "best_metrics": run_summary["best_metrics"],
                "history": run_summary["history"],
                "id2label": id2label,
            }

    # آمار میانگین‌ها
    def mean_std(x):
        return float(np.mean(x)), float(np.std(x))

    stats = {
        "train_loss_mean_std": mean_std(all_train_loss),
        "train_acc_mean_std":  mean_std(all_train_acc),
        "val_loss_mean_std":   mean_std(all_val_loss),
        "val_acc_mean_std":    mean_std(all_val_acc),
        "test_loss_mean_std":  mean_std(all_test_loss),
        "test_acc_mean_std":   mean_std(all_test_acc),
    }

    print("\n" + "#" * 80)
    print(f"خلاصه‌ی آزمایش {exp_name} روی {num_runs} ران:")
    print(f"Train loss mean±std: {stats['train_loss_mean_std'][0]:.4f} ± {stats['train_loss_mean_std'][1]:.4f}")
    print(f"Train acc  mean±std: {stats['train_acc_mean_std'][0]:.4f} ± {stats['train_acc_mean_std'][1]:.4f}")
    print(f"Val   loss mean±std: {stats['val_loss_mean_std'][0]:.4f} ± {stats['val_loss_mean_std'][1]:.4f}")
    print(f"Val   acc  mean±std: {stats['val_acc_mean_std'][0]:.4f} ± {stats['val_acc_mean_std'][1]:.4f}")
    print(f"Test  loss mean±std: {stats['test_loss_mean_std'][0]:.4f} ± {stats['test_loss_mean_std'][1]:.4f}")
    print(f"Test  acc  mean±std: {stats['test_acc_mean_std'][0]:.4f} ± {stats['test_acc_mean_std'][1]:.4f}")
    print(f"بهترین ران: {best_overall_run_idx} با val_loss = {best_overall_val_loss:.4f}")

    if save_pickle and best_overall is not None:
        os.makedirs("saved_models", exist_ok=True)
        fname = os.path.join("saved_models", f"best_model_{exp_name}.pkl")
        with open(fname, "wb") as f:
            pickle.dump(best_overall, f)
        print(f"مدل و سایر اطلاعات در '{fname}' ذخیره شد.")

    return stats, best_overall

"""## 3- config"""

BASE_CONFIG = {
    "use_normalization": True,
    "hidden1": 512,
    "hidden2": 256,
    "dropout_p1": 0.3,
    "dropout_p2": 0.3,
    "batch_size": 32,
    "num_epochs": 15,
    "lr": 1e-3,
    "random_labels_for_train_val": False,
    "add_gaussian_noise_to_test": False,
    "noise_std": 1.0,
    "seed_base": 42,
}

NUM_RUNS = 20  #    برای تست می‌توان موقتاً کم کنیم

"""## 4- Run:"""

exp1_config = deepcopy(BASE_CONFIG)

stats_exp1, best_exp1 = run_experiment("exp1_base", exp1_config, num_runs=NUM_RUNS)

exp2_config = deepcopy(BASE_CONFIG)
exp2_config["dropout_p1"] = 0.6
exp2_config["dropout_p2"] = 0.6

stats_exp2, best_exp2 = run_experiment("exp2_high_dropout", exp2_config, num_runs=NUM_RUNS)

exp3_config = deepcopy(BASE_CONFIG)
exp3_config["use_normalization"] = False

stats_exp3, best_exp3 = run_experiment("exp3_no_normalization", exp3_config, num_runs=NUM_RUNS)

exp4_config = deepcopy(BASE_CONFIG)
exp4_config["random_labels_for_train_val"] = True

stats_exp4, best_exp4 = run_experiment("exp4_random_labels", exp4_config, num_runs=NUM_RUNS)

exp5_config = deepcopy(BASE_CONFIG)
exp5_config["add_gaussian_noise_to_test"] = True
exp5_config["noise_std"] = 1.0  # طبق سؤال

stats_exp5, best_exp5 = run_experiment("exp5_noisy_test", exp5_config, num_runs=NUM_RUNS)

"""## 5- TTA"""

# چند ترنسفورم ملایم برای TTA روی فضای تانسور (پس از نرمال‌سازی)
# این ترنسفورم‌ها باید روی tensor عمل کنند، پس از torchvision.transforms.Random* استفاده می‌کنیم.
tta_transforms = [
    transforms.Compose([]),  # بدون تغییر
    transforms.Compose([transforms.RandomHorizontalFlip(p=1.0)]),
    transforms.Compose([transforms.GaussianBlur(kernel_size=3)]),
    transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.GaussianBlur(kernel_size=3),
    ]),
    transforms.Compose([
        transforms.RandomAdjustSharpness(sharpness_factor=0.8, p=1.0),
    ]),
]

# بازسازی مدل برای آزمایش ۵ از pickle
best5_fname = os.path.join("saved_models", "best_model_exp5_noisy_test.pkl")
with open(best5_fname, "rb") as f:
    best5_data = pickle.load(f)

model5 = BeanMLP(
    input_dim=FLATTEN_DIM,
    hidden1=best5_data["config"]["hidden1"],
    hidden2=best5_data["config"]["hidden2"],
    num_classes=NUM_CLASSES,
    p1=best5_data["config"]["dropout_p1"],
    p2=best5_data["config"]["dropout_p2"],
).to(device)
model5.load_state_dict(best5_data["model_state_dict"])
model5.eval()

# DataLoader با نویز روی test
train_loader_5, val_loader_5, test_loader_5 = make_dataloaders(
    use_normalization=True,
    batch_size=BASE_CONFIG["batch_size"],
    random_labels_for_train_val=False,
    add_gaussian_noise_to_test=True,
    noise_std=1.0,
)

criterion = nn.CrossEntropyLoss()

# ارزیابی بدون TTA
no_tta_loss, no_tta_acc = evaluate(model5, test_loader_5, criterion)
print(f"آزمون ۵ - بدون TTA: test_loss={no_tta_loss:.4f}, test_acc={no_tta_acc:.4f}")

# ارزیابی با TTA
tta_loss, tta_acc = evaluate_with_tta(model5, test_loader_5, criterion,
                                      tta_transforms=tta_transforms,
                                      n_augmentations=len(tta_transforms))
print(f"آزمون ۵ - با TTA:    test_loss={tta_loss:.4f}, test_acc={tta_acc:.4f}")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir runs

"""### my test learn"""